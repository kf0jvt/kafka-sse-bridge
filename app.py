from kafka import KafkaConsumer
from flask import Flask, Response, send_from_directory
import json
import queue
import threading
import sys
import os
import logging
from dotenv import load_dotenv
from dataclasses import dataclass
from typing import Generator

# Configure a general logger for libraries like KafkaConsumer
# and a custom logger for messages generated by this app
logging.basicConfig(
    level=logging.WARNING, format="%(asctime)s [%(levelname)s] %(funcName)s: %(message)s"
)
log_level = os.getenv('LOG_LEVEL', 'INFO').upper()
numeric_level = getattr(logging, log_level, logging.INFO)
logger = logging.getLogger(__name__)
logger.setLevel(numeric_level)

# Queue to pass messages from Kafka consumer to SSE clients
message_queue = queue.Queue()   

@dataclass
class KafkaConfig:
    consumer_bootstrap_1: list[str]
    consumer_bootstrap_2: list[str]
    kafka_topic: str
    keystore_password: str
    ssl_cert_file: str
    ssl_key_file: str
    ssl_ca_file: str

app = Flask(__name__)

def evaluate_impersonate(message: dict) -> dict:
    """
    Evaluate if a message indicates impersonation and add analysis.
        
    Returns:
        Dictionary with updated analysis if impersonation detected
    """
    
    # Check if message has _is_impersonating set to true
    if message.get("_is_impersonating", "false") == "true":
        message.setdefault('analysis', []).append('impersonation')
    
    return message

def evaluate_internal_api(message: dict) -> dict:
    """
    Evaluate if a message indicates impersonation and add analysis.
        
    Returns:
        Dictionary with updated analysis if impersonation detected
    """
    # Check if message has an internal ip address
    internal_ip = message.get("remote_ip", "").startswith("10.")

    # Check if message url is an api endpoint
    api_endpoint = any([message.get("url","").startswith("/api/now/v1"),
                        message.get("url","").startswith("api/sn_kmf")
    ])

    if internal_ip and api_endpoint:
        message.setdefault('analysis', []).append('internal_api')
    
    return message

# Define a list of evaluator functions that will enrich messages received from Kafka
EVALUATORS = [
    evaluate_internal_api,
    evaluate_impersonate,
]

def kafka_consumer_thread(kafka_config: KafkaConfig, broker: str) -> None:
    """Background thread to consume Kafka messages"""
    consumer = KafkaConsumer(
        kafka_config.kafka_topic,
        bootstrap_servers=getattr(kafka_config, broker),
        security_protocol='SSL',
        ssl_certfile=kafka_config.ssl_cert_file,
        ssl_keyfile=kafka_config.ssl_key_file,
        ssl_cafile=kafka_config.ssl_ca_file,
        auto_offset_reset='latest',
        enable_auto_commit=False, # read only connection
        group_id=None,
        value_deserializer=lambda m: m.decode('utf-8')
    )
    
    logger.info(f"Connected to Kafka broker {broker}s, subscribed to topic: {kafka_config.kafka_topic}")
    
    for message in consumer:
        logger.debug(f"Received message: {message.value}")

        message_parsed = json.loads(message.value)

        # Apply all evaluators in sequence
        for evaluator in EVALUATORS:
            message_parsed = evaluator(message_parsed)

        # Put message in queue for all SSE clients
        message_queue.put(message_parsed)

def event_stream() -> Generator[str, None, None]:
    """Generator function for SSE events"""
    while True:
        # Block until a message is available
        message = message_queue.get()
        # Format as SSE
        yield f"data: {json.dumps(message)}\n\n"

@app.route('/')
def index() -> Response:
    return send_from_directory('.', 'index.html')

@app.route('/static/<path:filename>')
def static_files(filename: str) -> Response:
    return send_from_directory('static', filename)

@app.route('/events')
def sse() -> Response:
    """SSE endpoint for clients to connect"""
    return Response(event_stream(), mimetype='text/event-stream')

@app.route('/health')
def health() -> Response:
    """Health check endpoint"""
    topic = app.config.get('KAFKA_TOPIC')
    return {'status': 'ok', 'kafka_topic': topic}

if __name__ == '__main__':
    # Load environment from .env file
    load_dotenv()

    # Check for necessary minimum information
    unrecoverable_error = False
    for variable_name in ["CONSUMER_BOOTSTRAP_1", 
                          "CONSUMER_BOOTSTRAP_2", 
                          "KAFKA_TOPIC",
                          "SSL_CERT_FILE",
                          "SSL_KEY_FILE",
                          "SSL_CA_FILE"]:
        if not os.getenv(variable_name, None):
            logger.critical(f"{variable_name} is not found in system ENV or in .env file. Cannot continue")
            unrecoverable_error = True
            continue
        if os.getenv(variable_name) == "":
            logger.critical(f"{variable_name} is set to an empty string. Cannot continue")
            unrecoverable_error = True
    if unrecoverable_error:
        sys.exit(1)

    consumer_bootstrap_1 = os.getenv("CONSUMER_BOOTSTRAP_1").split(",")
    consumer_bootstrap_2 = os.getenv("CONSUMER_BOOTSTRAP_2").split(",")
    kafka_topic = os.getenv("KAFKA_TOPIC")
    keystore_password = os.getenv("KEYSTORE_PASSWORD")
    ssl_cert_file = os.getenv("SSL_CERT_FILE")
    ssl_key_file = os.getenv("SSL_KEY_FILE")
    ssl_ca_file = os.getenv("SSL_CA_FILE")

    # make sure files are present and send a human-understandable error if they are not
    for each_file in [ssl_cert_file, ssl_key_file, ssl_ca_file]:
        if not os.path.exists(each_file):
            logger.critical(f"{each_file} is specified in .env configuration but is not found in this directory. Cannot continue")
            unrecoverable_error = True
    if unrecoverable_error:
        sys.exit(1)

    kafka_config = KafkaConfig(consumer_bootstrap_1, 
                               consumer_bootstrap_2, 
                               kafka_topic,
                               keystore_password, 
                               ssl_cert_file, 
                               ssl_key_file, 
                               ssl_ca_file)

    # Start Kafka consumers in background threads
    # ServiceNow Hermes publishes two consumers and can switch between them
    # For redundancy. We need to monitor both.
    consumer_ids = ["consumer_bootstrap_1", "consumer_bootstrap_2"]
    consumer_threads = [
        threading.Thread(
            target=kafka_consumer_thread,
            args=(kafka_config, consumer_id),
            daemon=True
        ).start()
        for consumer_id in consumer_ids
    ]
    
    app.config['KAFKA_TOPIC'] = kafka_config.kafka_topic

    # Start Flask server
    print("Starting SSE server on http://0.0.0.0:8000")
    print("Connect to http://localhost:8000/ to receive messages")
    app.run(host='0.0.0.0', port=8000, threaded=True)
